{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenising the titles \n",
    "We will now decapatilise and tokenise(segment) the titles so that we can process them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing df of story rows rather that comments \n",
    "# df = pd.read_csv('output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned df ; no non-english titles, no punctuation, lower cased, with columns for year and domain too\n",
    "# new columns are title_cleaned , domain , year, score, id, descendants\n",
    "# removed NaN titles\n",
    "df = pd.read_csv('output_newest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "#so we have 1.875247 million titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_cleaned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['title_cleaned'])\n",
    "print(df.shape)\n",
    "\n",
    "# this makes the titles decrease to 1.875237 mil titles so clearly the PSQL drop na didn't capture all the false titles, this is fine tho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_percent_size = int(len(df) * 0.2)\n",
    "other_80_percent_size = len(df) - int(len(df) * 0.2)\n",
    "\n",
    "titles_20 = df[\"title_cleaned\"].head(top_20_percent_size)\n",
    "titles_80 = df[top_20_percent_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old code for just taking top 2500 titles \n",
    "\n",
    "# df = df.head(2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old code for getting rid of non-english titles\n",
    "\n",
    "# from langdetect import detect, DetectorFactory\n",
    "# from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "# # Set seed for consistent language detection results\n",
    "# DetectorFactory.seed = 0\n",
    "\n",
    "# # Function to check if the title is in English\n",
    "# def is_english(text):\n",
    "#     try:\n",
    "#         return detect(text) == 'en'\n",
    "#     except LangDetectException:\n",
    "#         return False  # If detection fails, we assume it's not English\n",
    "\n",
    "# # Filter rows where the title is in English\n",
    "# df['title'] = df['title'].str.lower()\n",
    "# df = df[df['title'].apply(is_english)]\n",
    "\n",
    "# # Display the modified DataFrame\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old code for stemming and lemmatizing, removing punctuation etc.\n",
    "\n",
    "# # import natural language toolkit\n",
    "# import nltk\n",
    "# from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# # Download NLTK resources if you haven't done so already\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# # Initialize stemmer and lemmatizer\n",
    "# stemmer = PorterStemmer()\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# ###################################################################################################\n",
    "\n",
    "# # Function to tokenize and apply stemming/lemmatization\n",
    "# def tokenize_and_process(text):\n",
    "#     if not isinstance(text, str) or not text:  # Check if the text is a valid non-empty string\n",
    "#         return ''  # Return empty string for non-string or empty values\n",
    "\n",
    "#     # Tokenize the text by whitespace\n",
    "#     tokens = text.split()\n",
    "    \n",
    "#     # Apply stemming and lemmatization\n",
    "#     processed_tokens = [lemmatizer.lemmatize(stemmer.stem(token)) for token in tokens]\n",
    "    \n",
    "#     return ' '.join(processed_tokens)  # Return the processed tokens as a single string\n",
    "# ###################################################################################################\n",
    "\n",
    "# # Function to tokenize without stemming/lemmatization (for testing cosine similarity purposes)\n",
    "# # def tokenize_and_process(text):\n",
    "# #     if not isinstance(text, str) or not text:  # Check if the text is a valid non-empty string\n",
    "# #         return ''  # Return empty string for non-string or empty values\n",
    "\n",
    "# #     # Tokenize the text by whitespace\n",
    "# #     tokens = text.split()\n",
    "    \n",
    "# #     return ' '.join(tokens)  # Return the tokens as a single string\n",
    "# ###################################################################################################\n",
    "\n",
    "# # Apply the function to title column \n",
    "# df['title'] = df['title'].apply(tokenize_and_process)\n",
    "\n",
    "# # Apply the function to the 'adjusted_title' column if we want to do stuff like remove punctuation for one too\n",
    "# # df['adjusted_title'] = df['title'].apply(tokenize_and_process)\n",
    "\n",
    "# # Display the modified DataFrame\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old code for Tokenizing titles \n",
    "\n",
    "import re\n",
    "\n",
    "# Old code for replacing punctuation with special tokens and then tokenising ////////////////////////////////////////////\n",
    "\n",
    "# def preprocess_text(text):\n",
    "#     # Replace various punctuation with special tokens\n",
    "#     replacements = {\n",
    "#         ',': '<COMMA>',\n",
    "#         '.': '<FULLSTOP>',\n",
    "#         '!': '<EXCLAMATION>',\n",
    "#         '?': '<QUESTION>',\n",
    "#         ';': '<SEMICOLON>',\n",
    "#         ':': '<COLON>',\n",
    "#         '(': '<OPENPAREN>',\n",
    "#         ')': '<CLOSEPAREN>',\n",
    "#         '[': '<OPENBRACKET>',\n",
    "#         ']': '<CLOSEBRACKET>',\n",
    "#         '{': '<OPENBRACE>',\n",
    "#         '}': '<CLOSEBRACE>',\n",
    "#         '\\'': '<QUOTE>',\n",
    "#         '\\\"': '<DOUBLEQUOTE>',\n",
    "#         '-': '<DASH>',\n",
    "#         '_': '<UNDERLINE>',\n",
    "#         'â€¦': '<ELLIPSIS>'  # Ellipsis\n",
    "#     }\n",
    "#     for punctuation, token in replacements.items():\n",
    "#         text = text.replace(punctuation, token)  # Replace each punctuation\n",
    "#     return text\n",
    "# # Tokenize the text (split into individual words and special tokens)\n",
    "# def tokenize_text(text):\n",
    "#     # Tokenize by splitting on whitespace and keeping punctuation tokens\n",
    "#     return re.findall(r'\\S+|<COMMA>|<FULLSTOP>|<EXCLAMATION>|<QUESTION>|<SEMICOLON>|<COLON>|<OPENPAREN>|<CLOSEPAREN>|<OPENBRACKET>|<CLOSEBRACKET>|<OPENBRACE>|<CLOSEBRACE>|<QUOTE>|<DOUBLEQUOTE>|<DASH>|<UNDERLINE>|<ELLIPSIS>', text)\n",
    "# # Preprocess and tokenize the 'title' column\n",
    "# df['title'] = df['title'].apply(preprocess_text)\n",
    "# df['tokens'] = df['title'].apply(tokenize_text)\n",
    "\n",
    "\n",
    "# Old code for tokenising without punctuation tokens ////////////////////////////////////////\n",
    "\n",
    "# # Tokenize the text (split into individual words)\n",
    "# def tokenize_text(text):\n",
    "#     # Tokenize by splitting on whitespace\n",
    "#     return re.findall(r'\\S+', text)\n",
    "\n",
    "# # Preprocess and tokenize the 'title_cleaned' column\n",
    "# df['tokens'] = df['title_cleaned'].apply(tokenize_text)\n",
    "\n",
    "# # Create a titles vocabulary (a dictionary mapping tokens from titles to unique IDs)\n",
    "# def create_vocabulary(tokens_series):\n",
    "#     # Flatten the list of token lists into a single list\n",
    "#     all_tokens = [token for tokens in tokens_series for token in tokens]\n",
    "#     # Create a unique token list and assign an ID to each\n",
    "#     vocabulary = {token: idx + 1 for idx, token in enumerate(sorted(set(all_tokens)))}\n",
    "#     return vocabulary\n",
    "\n",
    "# # Create vocabulary (a dictionary) from the tokens in the DataFrame\n",
    "# vocab = create_vocabulary(df['tokens'])\n",
    "\n",
    "\n",
    "\n",
    "# # Function to convert tokens to token IDs using the vocabulary\n",
    "# # def tokens_to_ids(tokens, vocab):\n",
    "# #     return [vocab[token] for token in tokens]\n",
    "# def tokens_to_ids(token, vocab):\n",
    "#     return [vocab[token]]\n",
    "\n",
    "# # Apply the function to convert tokens to token IDs\n",
    "# # df['token_ids'] = df['tokens'].apply(lambda tokens: tokens_to_ids(tokens, vocab))\n",
    "\n",
    "# # Display the DataFrame with token IDs\n",
    "# print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE THIS Update the text8 dictionary with 20% of our tokenised, cleaned titles \n",
    "\n",
    "# def update_our_dict(our_dict: dict[int, str], titles_20: list[str]) -> dict:\n",
    "#     # Flatten the list of titles into a list of individual words\n",
    "#     all_words = [word for title in titles_20 for word in title.split()]\n",
    "    \n",
    "#     # Reverse the our_dict to map words to their current IDs (for easier lookup)\n",
    "#     words_to_ids = {word: idx for idx, word in our_dict.items()}\n",
    "    \n",
    "#     # Start assigning new IDs after the maximum ID in our_dict\n",
    "#     if our_dict:\n",
    "#         next_id = max(our_dict.keys()) + 1\n",
    "#     else:\n",
    "#         next_id = 1  # Start from 1 if our_dict is empty\n",
    "    \n",
    "#     # Iterate over all words and add new words to our_dict\n",
    "#     for word in all_words:\n",
    "#         if word not in words_to_ids:  # If the word is not already in the dict\n",
    "#             our_dict[next_id] = word\n",
    "#             words_to_ids[word] = next_id  # Update the reverse mapping\n",
    "#             next_id += 1\n",
    "    \n",
    "#     return our_dict\n",
    "\n",
    "# updated_dict_20 = update_our_dict(our_dict, titles_20)\n",
    "# print(updated_dict_20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search word in dictionary for ID\n",
    "\n",
    "def search_word_in_dict(word: str, dictionary: dict[int, str]) -> int:\n",
    "    # Iterate over dictionary items (ID and word)\n",
    "    for id, dict_word in dictionary.items():\n",
    "        if dict_word == word:  # Check if the word matches\n",
    "            return id  # Return the ID of the word if found\n",
    "    return None  # Return None if the word is not found\n",
    "\n",
    "# Search for any word\n",
    "word_to_search = 'the'\n",
    "id_of_word = search_word_in_dict(word_to_search, updated_dict_20)\n",
    "\n",
    "if id_of_word is not None:\n",
    "    print(f'The ID for the word \"{word_to_search}\" is {id_of_word}.')\n",
    "else:\n",
    "    print(f'The word \"{word_to_search}\" is not in the dictionary.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore this\n",
    "\n",
    "# with open('updated_dict_20.pkl', 'wb') as fp:\n",
    "#     pickle.dump(updated_dict_20, fp)\n",
    "#     print('dictionary saved successfully to file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# titles_20.to_csv(\"titles_20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send titles_20 to Omar so he can create dictionary -> he will send dictionary back to me so i cna remove titles from titles_80 which aren't \n",
    "# in the dictionary.\n",
    "\n",
    "titles_20 = pd.DataFrame(titles_20, columns=['title_cleaned'])\n",
    "titles_20.to_csv(\"titles_20_scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing 'our_dict' from Omar which is a dictionary with IDs trained on the text8 file and titles_20 file\n",
    "# Run from here next\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('vocab_dict_newest.pkl', 'rb') as fp:\n",
    "    our_dict = pickle.load(fp)\n",
    "    print('Our Dictionary')\n",
    "    print(our_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_invalid_titles(titles_80: pd.DataFrame, title_column: str, our_dict: dict[int, str]) -> pd.DataFrame:\n",
    "    # Reverse our_dict to get a dictionary {word: ID}\n",
    "    words_to_ids = {word: id for id, word in our_dict.items()}\n",
    "    \n",
    "    # # Filter the titles: Keep only titles where all words exist in our_dict\n",
    "    # valid_titles = []\n",
    "    # for title in titles_80:\n",
    "    #     words = title.split()  # Split the title into individual words\n",
    "    #     if all(word in words_to_ids for word in words):  # Check if all words are in vocab_dict\n",
    "    #         valid_titles.append(title)  # If valid, add to the list of valid titles\n",
    "    \n",
    "    # return valid_titles\n",
    "\n",
    "    # Filter the titles in the specified column: Keep only titles where all words exist in our_dict\n",
    "    valid_rows = []\n",
    "    \n",
    "    for index, row in titles_80.iterrows():\n",
    "        title = row[title_column]  # Access the specific column containing titles\n",
    "        words = title.split()  # Split the title into individual words\n",
    "        \n",
    "        # Check if all words in the title are present in the our_dict vocabulary\n",
    "        if all(word in words_to_ids for word in words):\n",
    "            valid_rows.append(row)  # If valid, add the entire row to the list of valid rows\n",
    "    \n",
    "    # Create a new DataFrame with only the valid rows\n",
    "    valid_titles_df = pd.DataFrame(valid_rows, columns=titles_80.columns)\n",
    "    \n",
    "    return valid_titles_df\n",
    "\n",
    "# Remove invalid titles\n",
    "valid_titles_80 = remove_invalid_titles(titles_80, 'title_cleaned', our_dict)\n",
    "\n",
    "\n",
    "# def remove_invalid_titles_from_df(titles_df: pd.DataFrame, title_column: str, our_dict: dict[int, str]) -> pd.DataFrame:\n",
    "#     # Reverse our_dict to get a dictionary {word: ID}\n",
    "#     words_to_ids = {word: id for id, word in our_dict.items()}\n",
    "    \n",
    "#     # Filter the titles in the specified column: Keep only titles where all words exist in our_dict\n",
    "#     valid_rows = []\n",
    "    \n",
    "#     for index, row in titles_df.iterrows():\n",
    "#         title = row[title_column]  # Access the specific column containing titles\n",
    "#         words = title.split()  # Split the title into individual words\n",
    "        \n",
    "#         # Check if all words in the title are present in the our_dict vocabulary\n",
    "#         if all(word in words_to_ids for word in words):\n",
    "#             valid_rows.append(row)  # If valid, add the entire row to the list of valid rows\n",
    "    \n",
    "#     # Create a new DataFrame with only the valid rows\n",
    "#     valid_titles_df = pd.DataFrame(valid_rows, columns=titles_df.columns)\n",
    "    \n",
    "#     return valid_titles_df\n",
    "\n",
    "# # Example usage:\n",
    "# # Assume 'titles_80' is a DataFrame with a 'title_column_name' that contains the titles you want to filter\n",
    "# title_column_name = 'title_cleaned'  # The column in the DataFrame containing the titles\n",
    "# valid_titles_80_df = remove_invalid_titles_from_df(titles_80, title_column_name, our_dict)\n",
    "\n",
    "# # Print the result\n",
    "# print(valid_titles_80_df)\n",
    "\n",
    "# # Optionally, save the filtered DataFrame to a new CSV\n",
    "# valid_titles_80_df.to_csv('valid_titles_80.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_titles_80.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_titles_80.to_csv(\"valid_titles_80_scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merging the old valid_titles to incorporate score data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_valid_titles.loc[1227448]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Load the CSV files into pandas DataFrames\n",
    "# df_output_newest = pd.read_csv('output_newest.csv')\n",
    "# df_valid_titles = pd.read_csv('valid_titles')\n",
    "\n",
    "# # Perform the left join, ensuring all rows in df_valid_titles are retained\n",
    "# # Merge strictly on 'valid_titles' from df_valid_titles and 'title_cleaned' from df_output_newest\n",
    "# merged_df = pd.merge(df_valid_titles, df_output_newest, left_on='valid_titles', right_on='title_cleaned', how='left')\n",
    "\n",
    "# # Print the result to check\n",
    "# print(merged_df)\n",
    "\n",
    "# # Optionally, save the merged result to a new CSV file\n",
    "# merged_df.to_csv('merged_output.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was our sentencepiece tokenization effort. This has been put on the backburner.\n",
    "\n",
    "#import pandas as pd\n",
    "#import sentencepiece as spm\n",
    "\n",
    "# Step 1: Preprocess the text (optional)\n",
    "# def preprocess_text(text):\n",
    "#     # Replace specific punctuation with special tokens\n",
    "#     text = text.replace(',', '<COMMA>')  # Replace commas\n",
    "#     text = text.replace('.', '<FULLSTOP>')  # Replace full stops\n",
    "#     text = text.replace('!', '<EXCLAMATION>')  # Replace exclamations\n",
    "#     return text\n",
    "\n",
    "# # Preprocess the 'title' column\n",
    "# df['title'] = df['title'].apply(preprocess_text)\n",
    "\n",
    "# # Step 2: Save titles to a file for training SentencePiece\n",
    "# with open('titles.txt', 'w') as f:\n",
    "#     for title in df['title']:\n",
    "#         f.write(title + '\\n')\n",
    "\n",
    "# # Step 3: Train a SentencePiece model\n",
    "# # Increase vocab_size to at least 100 to avoid the error\n",
    "# spm.SentencePieceTrainer.train('--input=titles.txt --model_prefix=m --vocab_size=100 --character_coverage=1.0')\n",
    "\n",
    "# # Step 4: Load the trained SentencePiece model\n",
    "# sp = spm.SentencePieceProcessor(model_file='m.model')\n",
    "\n",
    "# # Tokenize the 'title' column using SentencePiece\n",
    "# df['tokens'] = df['title'].apply(lambda x: sp.encode(x, out_type=str))\n",
    "\n",
    "# # Create a vocabulary from the tokens\n",
    "# def create_vocabulary(tokens_series):\n",
    "#     all_tokens = [token for tokens in tokens_series for token in tokens]\n",
    "#     vocabulary = {token: idx + 1 for idx, token in enumerate(sorted(set(all_tokens)))}\n",
    "#     return vocabulary\n",
    "\n",
    "# # Get vocabulary from the tokens in the DataFrame\n",
    "# vocab = create_vocabulary(df['tokens'])\n",
    "\n",
    "# # Function to convert tokens to token IDs using the vocabulary\n",
    "# def tokens_to_ids(tokens, vocab):\n",
    "#     return [vocab[token] for token in tokens]\n",
    "\n",
    "# # Apply the function to convert tokens to token IDs\n",
    "# df['token_ids'] = df['tokens'].apply(lambda tokens: tokens_to_ids(tokens, vocab))\n",
    "\n",
    "# # Display the DataFrame with tokens and token IDs\n",
    "# print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embedding \n",
    "\n",
    "We now need to get the embedding first we will analyze the embedding process for the wikipedia text before "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to stem, lemmatize text8 and run gensim model on text8 (for embedding)\n",
    "    \n",
    "import pandas as pd\n",
    "import re\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Download the required NLTK resources if you haven't already\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Replace specific punctuation with special tokens\n",
    "    replacements = {\n",
    "        ',': '<COMMA>',\n",
    "        '.': '<FULLSTOP>',\n",
    "        '!': '<EXCLAMATION>',\n",
    "        '?': '<QUESTION>',\n",
    "        ';': '<SEMICOLON>',\n",
    "        ':': '<COLON>',\n",
    "        '(': '<OPENPAREN>',\n",
    "        ')': '<CLOSEPAREN>',\n",
    "        '[': '<OPENBRACKET>',\n",
    "        ']': '<CLOSEBRACKET>',\n",
    "        '{': '<OPENBRACE>',\n",
    "        '}': '<CLOSEBRACE>',\n",
    "        '\\'': '<QUOTE>',\n",
    "        '\\\"': '<DOUBLEQUOTE>',\n",
    "        '-': '<DASH>',\n",
    "        '_': '<UNDERLINE>',\n",
    "        'â€¦': '<ELLIPSIS>'  # Ellipsis\n",
    "    }\n",
    "\n",
    "    # Replace punctuation with special tokens\n",
    "    for punctuation, token in replacements.items():\n",
    "        text = text.replace(punctuation, token)\n",
    "\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Function to tokenize and apply stemming/lemmatization\n",
    "def tokenize_and_process(text):\n",
    "    # Tokenize the text by whitespace\n",
    "    tokens = text.split()\n",
    "    # Apply stemming or lemmatization\n",
    "    processed_tokens = [lemmatizer.lemmatize(stemmer.stem(token)) for token in tokens]\n",
    "    return processed_tokens\n",
    "\n",
    "# Function to tokenize without stemming/lemmatization\n",
    "# def tokenize_and_process(text):\n",
    "#     # Tokenize the text by whitespace\n",
    "#     tokens = text.split()\n",
    "    \n",
    "#     return tokens\n",
    "\n",
    "\n",
    "# Read the text file\n",
    "file_path = 'text8.txt'  # Replace with your file path\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Preprocess the content\n",
    "preprocessed_content = preprocess_text(content)\n",
    "\n",
    "# Tokenize and process the preprocessed text\n",
    "tokens = tokenize_and_process(preprocessed_content)\n",
    "\n",
    "# Create sentences for Word2Vec\n",
    "sentences = [tokens]  # Wrap tokens in a list to form a single sentence\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4, sg=0)\n",
    "\n",
    "# sentences = list of tokens that the model will be trained on. Words close to eachother will be close in meaning because it's a list\n",
    "#             of sequential words from sentences. \n",
    "# vectorsize = size of the embedding (how many dimensions of 'meaning' we have)\n",
    "# window = amount of context taken into account when predicting words\n",
    "# min_count = we ignore words which are below a certain count. so ofc usually above 1.\n",
    "# workers = for the sake of parallelizing the task. CPUs are multicore.\n",
    "# sg = 0 if you want CBOW. 1 if you want skipgram.\n",
    "\n",
    "# Save the model\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: getting the vector for a word\n",
    "word_vector = model.wv['exampl']  # Replace 'example' with any word from your corpus\n",
    "\n",
    "print(word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load your pre-trained Word2Vec model or train one\n",
    "# model = Word2Vec.load(\"your_model_path\")  # Load an existing model\n",
    "# Or train your model\n",
    "# model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Example word pairs and their human-annotated similarity scores\n",
    "word_pairs = [('king', 'queen'), ('man', 'woman')]  # Example pairs\n",
    "human_scores = [0.8, 0.9]  # Human-annotated scores\n",
    "\n",
    "# Compute cosine similarities\n",
    "similarities = []\n",
    "for w1, w2 in word_pairs:\n",
    "    try:\n",
    "        vec1 = model.wv[w1]  # Get the vector for the first word\n",
    "        vec2 = model.wv[w2]  # Get the vector for the second word\n",
    "        cosine_similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "        similarities.append(cosine_similarity)\n",
    "    except KeyError as e:\n",
    "        print(f\"Word not in vocabulary: {e}\")\n",
    "\n",
    "# Calculate Spearman correlation\n",
    "if similarities:  # Ensure there are similarities to compare\n",
    "    corr, _ = spearmanr(human_scores, similarities)\n",
    "    print(f'Spearman correlation: {corr}')\n",
    "else:\n",
    "    print(\"No valid similarities calculated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get word vectors for all tokens in the first row\n",
    "first_row_vectors = [model.wv[token] for token in df['tokens'][10] if token in model.wv]\n",
    "\n",
    "# Display word vectors\n",
    "print(\"Word Vectors for the first row tokens:\")\n",
    "for token, vector in zip(df['tokens'][500], first_row_vectors):\n",
    "    print(f\"Token: {token}, Vector: {vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load your trained Word2Vec model\n",
    "# model = Word2Vec.load(\"your_model_path\")  # Uncomment to load an existing model\n",
    "\n",
    "# Function to predict a target word using CBOW\n",
    "def predict_target_word(context_words, model):\n",
    "    # Filter out words not in the model's vocabulary\n",
    "    context_vectors = []\n",
    "    for word in context_words:\n",
    "        if word in model.wv:\n",
    "            context_vectors.append(model.wv[word])\n",
    "        else:\n",
    "            print(f\"Word '{word}' not in vocabulary.\")\n",
    "\n",
    "    # Check if we have valid context vectors\n",
    "    if not context_vectors:\n",
    "        return None\n",
    "    \n",
    "    # Average the context word vectors\n",
    "    context_vector = np.mean(context_vectors, axis=0)\n",
    "\n",
    "    # Find the most similar word to the context vector\n",
    "    similar_words = model.wv.similar_by_vector(context_vector, topn=5)  # Get top 5 similar words\n",
    "\n",
    "    return similar_words\n",
    "\n",
    "# Example sentence and context related to chloroplasts\n",
    "target_phrase = \"chloroplasts are vital for photosynthesis\"\n",
    "context_words = [\"chloroplasts\", \"are\", \"vital\", \"for\", \"photosynthesis\"]\n",
    "\n",
    "# Predicting the target word\n",
    "predicted_words = predict_target_word(context_words, model)\n",
    "\n",
    "# Display predicted words\n",
    "if predicted_words:\n",
    "    print(\"Predicted words for context:\", predicted_words)\n",
    "else:\n",
    "    print(\"No valid predictions could be made.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Create a mapping from tokens to their indices\n",
    "token_to_index = {token: idx for idx, token in enumerate(set(tokens))}\n",
    "index_to_token = {idx: token for token, idx in token_to_index.items()}\n",
    "\n",
    "# Convert tokens to their corresponding indices\n",
    "numerical_data = [token_to_index[token] for token in tokens]\n",
    "\n",
    "# Prepare training samples (you can use a sliding window approach or any method suitable for your task)\n",
    "window_size = 2  # For context size\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(window_size, len(numerical_data) - window_size):\n",
    "    context = numerical_data[i - window_size:i] + numerical_data[i + 1:i + window_size + 1]\n",
    "    target = numerical_data[i]\n",
    "    X.append(context)\n",
    "    y.append(target)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.long), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = Word2VecDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Word2VecModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2VecModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, context):\n",
    "        # Get embeddings for context words\n",
    "        embeds = self.embeddings(context)\n",
    "        # Average the embeddings\n",
    "        avg_embed = embeds.mean(dim=1)\n",
    "        return self.linear(avg_embed)\n",
    "\n",
    "# Initialize the model\n",
    "vocab_size = len(token_to_index)\n",
    "embedding_dim = 100  # Same as Word2Vec vector size\n",
    "model = Word2VecModel(vocab_size, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # Import tqdm\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # Wrap your DataLoader with tqdm to show progress\n",
    "    with tqdm(total=len(dataloader), desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch') as pbar:\n",
    "        for context, target in dataloader:\n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "            output = model(context)  # Forward pass\n",
    "            loss = criterion(output, target)  # Compute loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "            \n",
    "            pbar.update(1)  # Update the progress bar\n",
    "            pbar.set_postfix(loss=loss.item())  # Optionally display loss value\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- -- FASTAPI SETUP -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxim\\AppData\\Local\\Temp\\ipykernel_26092\\1738045738.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  embedding_model = torch.load('skipgram_model_titles.pth', map_location=torch.device('cpu'))  # Adjust this for your model type\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import torch\n",
    "import re\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Load your pre-trained models\n",
    "# Model 1: Embedding model\n",
    "embedding_model = torch.load('skipgram_model_titles.pth', map_location=torch.device('cpu'))  # Adjust this for your model type\n",
    "\n",
    "\n",
    "# # Model 2: Prediction model (based on embeddings)\n",
    "# with open('prediction_model.pkl', 'rb') as f:\n",
    "#     prediction_model = pickle.load(f)\n",
    "\n",
    "\n",
    "# Define the request schema using Pydantic\n",
    "class TitleInput(BaseModel):\n",
    "    title: str\n",
    "\n",
    "# Preprocessing function to lowercase, remove punctuation, and tokenize the title\n",
    "def preprocess_title(title: str) -> str:\n",
    "    # Lowercase the title\n",
    "    title = title.lower()\n",
    "    # Remove punctuation using regex\n",
    "    title = re.sub(r'[^\\w\\s]', '', title)\n",
    "    # Tokenize the title (split into words)\n",
    "    tokens = title.split()\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Define the API endpoint for prediction\n",
    "@app.post('/predict')\n",
    "def predict_score(input_data: TitleInput):\n",
    "    title = input_data.title\n",
    "\n",
    "    # Remove punctuation, lowercase, tokenize\n",
    "    preprocessed_title = preprocess_title(title)\n",
    "\n",
    "    # Convert the title to an embedding using the first model\n",
    "    title_embedding = embedding_model.encode(preprocessed_title)  # Use the appropriate method for your model\n",
    "\n",
    "    # Predict the score using the second model\n",
    "    # title_embedding = np.array(title_embedding).reshape(1, -1)  # Ensure correct input shape\n",
    "    # score_prediction = prediction_model.predict(title_embedding)\n",
    "\n",
    "    # Return the predicted score\n",
    "    # return {\"predicted_score\": float(score_prediction[0])}\n",
    "\n",
    "    \n",
    "    return {\"embeddings\": title_embedding}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
